{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def findUserName (a):\n",
    "#driver.find_element_by_id('loginbutton').click()\n",
    "#time.sleep(3)\n",
    "#driver.get('https://user.qzone.qq.com/{}'.format('244161734'))\n",
    "#time.sleep(3)\n",
    "#html = driver.page_source\n",
    "#soup = BeautifulSoup(html,'lxml')\n",
    "#a = soup.find_all('div',{'class':'article'})\n",
    "#for i in range(len(a)):\n",
    "    #print(a[i].text)\n",
    "    headers = {\n",
    "        'User-Agent':'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_10_5) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/64.0.3282.186 Safari/537.36',\n",
    "    }\n",
    "    start_urls=a\n",
    "    response = requests.get(start_urls,headers = headers).text\n",
    "    soup = BeautifulSoup(response,'lxml')\n",
    "    for item in soup.find_all('span',class_='username u-dir u-textTruncate'):\n",
    "        print(item.b)\n",
    "    #href = item.['href']\n",
    "    #content = {\n",
    "    #            'href': item.a['href'],\n",
    "    #        }\n",
    "    #print(item.a['href'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://twitter.com/search?q=%23flu2018&src=typd\n",
      "<b>Destiny22Ginger</b>\n",
      "<b>ASharpGuy2</b>\n",
      "<b>ForRealzy</b>\n",
      "<b>kimbymarriage</b>\n",
      "<b>ClvrWhtvr</b>\n",
      "<b>awdahl14</b>\n",
      "<b>allthingssabs</b>\n",
      "<b>chloe_warfford</b>\n",
      "<b>Xingaling1274</b>\n",
      "<b>djmd28</b>\n",
      "<b>tracysky</b>\n",
      "<b>WriterJordyn</b>\n",
      "<b>eyelove808</b>\n",
      "<b>djmd28</b>\n",
      "<b>sarahhunsin</b>\n",
      "<b>eyelove808</b>\n",
      "<b>CT48507642</b>\n",
      "<b>djmd28</b>\n",
      "<b>Sunnyhaze2006</b>\n",
      "<b>katylovexo</b>\n"
     ]
    }
   ],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "from selenium import webdriver\n",
    "import time\n",
    "import requests \n",
    "driver = webdriver.Chrome(executable_path='/usr/bin/chromedriver.exe')\n",
    "driver.set_window_size(50000,20000)\n",
    "\n",
    "\n",
    "usr = \"liang10000\"\n",
    "pwd = \"Mengliang512\"\n",
    "#time.sleep(3)\n",
    "#driver.switch_to.frame('login_frame')\n",
    "#driver.find_element_by_id('switcher_plogin').click()\n",
    "driver.get('https://twitter.com/login')\n",
    "time.sleep(3)\n",
    "usr_box = driver.find_element_by_class_name('js-username-field')\n",
    "usr_box.send_keys(usr)\n",
    "pwd_box = driver.find_element_by_class_name('js-password-field')\n",
    "pwd_box.send_keys(pwd)\n",
    "\n",
    "login_button = driver.find_element_by_css_selector(\"button.submit.EdgeButton.EdgeButton--primary.EdgeButtom--medium\") \n",
    "login_button.submit()\n",
    "\n",
    "search_box = driver.find_element_by_id(\"search-query\")\n",
    "keyword = \"#flu2018\"\n",
    "search_box.send_keys(keyword)\n",
    "search = driver.find_element_by_css_selector('button.Icon.Icon--medium.Icon--search.nav-search')\n",
    "search.submit()\n",
    "time.sleep(3)\n",
    "a = driver.current_url\n",
    "print(a)\n",
    "#for i in range (1:1000):\n",
    "#for i in range (1,2):\n",
    "    #js=\"var q=document.documentElement.scrollTop=100000\"  \n",
    "    #driver.execute_script(js)  \n",
    "    #time.sleep(5)\n",
    "time.sleep(20)    \n",
    "findUserName(a)\n",
    "\n",
    " \n",
    "\n",
    "    \n",
    "     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<b>Destiny22Ginger</b>\n",
      "<b>ASharpGuy2</b>\n",
      "<b>ForRealzy</b>\n",
      "<b>ClvrWhtvr</b>\n",
      "<b>awdahl14</b>\n",
      "<b>ChristineforSCC</b>\n",
      "<b>SCCHealth</b>\n",
      "<b>kimbymarriage</b>\n",
      "<b>allthingssabs</b>\n",
      "<b>chloe_warfford</b>\n",
      "<b>Xingaling1274</b>\n",
      "<b>djmd28</b>\n",
      "<b>tracysky</b>\n",
      "<b>WriterJordyn</b>\n",
      "<b>eyelove808</b>\n",
      "<b>djmd28</b>\n",
      "<b>eyelove808</b>\n",
      "<b>CT48507642</b>\n",
      "<b>djmd28</b>\n",
      "<b>katylovexo</b>\n",
      "<b>HMDurand</b>\n"
     ]
    }
   ],
   "source": [
    "findUserName(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "#driver.find_element_by_id('loginbutton').click()\n",
    "#time.sleep(3)\n",
    "#driver.get('https://user.qzone.qq.com/{}'.format('244161734'))\n",
    "#time.sleep(3)\n",
    "#html = driver.page_source\n",
    "#soup = BeautifulSoup(html,'lxml')\n",
    "#a = soup.find_all('div',{'class':'article'})\n",
    "#for i in range(len(a)):\n",
    "    #print(a[i].text)\n",
    "headers = {\n",
    "    'User-Agent':'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_10_5) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/64.0.3282.186 Safari/537.36',\n",
    "}\n",
    "start_urls=a\n",
    "response = requests.get(start_urls,headers = headers).text\n",
    "soup = BeautifulSoup(response,'lxml')\n",
    "for item in soup.find_all('span',class_='username u-dir u-textTruncate'):\n",
    "    print(item.b)\n",
    "    #href = item.['href']\n",
    "    #content = {\n",
    "    #            'href': item.a['href'],\n",
    "    #        }\n",
    "    #print(item.a['href'])  \n",
    "thread.sleep(1000)\n",
    "findUserName(a)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import requests\n",
    "\n",
    "headers = {\n",
    "    'User-Agent':'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_10_5) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/64.0.3282.186 Safari/537.36',\n",
    "}\n",
    "url = \"https://twitter.com/login\"\n",
    "name= \"liang10000\"\n",
    "password = \"Mengliang512\"\n",
    "payload = { 'session[username_or_email]': name, \n",
    "            'session[password]': password}\n",
    "r = requests.post(url, data=payload)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print(r)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "someone=\"liang10000\"\n",
    "url = 'http://twitter.com/%s/followers'%(someone)\n",
    "p = requests.get(url, headers=headers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print(p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_detail_url():\n",
    "    start_urls=['http://www.ximalaya.com/dq/all/{}/'.format(pn) for pn in range (1,85)]\n",
    "    #read each url of the website \n",
    "    for start_url in start_urls:\n",
    "        response = requests.get(start_url,headers = headers).text\n",
    "        soup = BeautifulSoup(response,'lxml')\n",
    "        for item in soup.find_all('div',class_='discoverAlbum_item'):\n",
    "            href = item.a['href']\n",
    "            title = item.img['alt']\n",
    "            img_url = item.img['src']\n",
    "            content = {\n",
    "                'href': item.a['href'],\n",
    "                'title': item.img['alt'],\n",
    "                'img_url': item.img['src'],\n",
    "            }\n",
    "            \n",
    "            #print('downloading{}'.format(item.img['alt']))\n",
    "            print(item.a['href'])  \n",
    "            print(item.img['alt'])\n",
    "            get_mp3(href,title)\n",
    "            time.sleep(1)\n",
    "            break \n",
    "        break   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_detail_url():\n",
    "    start_urls=['http://www.ximalaya.com/dq/all/{}/'.format(pn) for pn in range (1,85)]\n",
    "    #read each url of the website \n",
    "    for start_url in start_urls:\n",
    "        response = requests.get(start_url,headers = headers).text\n",
    "        soup = BeautifulSoup(response,'lxml')\n",
    "        for item in soup.find_all('div',class_='discoverAlbum_item'):\n",
    "            href = item.a['href']\n",
    "            title = item.img['alt']\n",
    "            img_url = item.img['src']\n",
    "            content = {\n",
    "                'href': item.a['href'],\n",
    "                'title': item.img['alt'],\n",
    "                'img_url': item.img['src'],\n",
    "            }\n",
    "            \n",
    "            #print('downloading{}'.format(item.img['alt']))\n",
    "            print(item.a['href'])  \n",
    "            print(item.img['alt'])\n",
    "            get_mp3(href,title)\n",
    "            time.sleep(1)\n",
    "            break \n",
    "        break     \n",
    "             \n",
    "def get_mp3(url,title):\n",
    "    response = requests.get(url,headers = headers).text\n",
    "    num_list = etree.HTML(response).xpath('//div[@class=\"personal_body\"]/@sound_ids')[0].split(',')\n",
    "    print(len(num_list))\n",
    "    mkdir(title)\n",
    "    os.chdir(r'/Users/mtoasf/Desktop/xmly/'+title) #switch the path for saving file \n",
    "    for id in num_list:\n",
    "        json_url='http://www.ximalaya.com/tracks/{}.json'.format(id)\n",
    "        html = requests.get(json_url,headers = headers).json()\n",
    "        mp4_url = html.get('play_path')\n",
    "        download(mp4_url)\n",
    "        print(title+'downloaded')\n",
    "    \n",
    "def mkdir(title): \n",
    "    path = title.strip()\n",
    "    isExists = os.path.exists(os.path.join(r'/Users/mtoasf/Desktop/xmly/',path))\n",
    "    if not isExists:\n",
    "        print(u'get a 1 folder')\n",
    "        os.makedirs(os.path.join(r'/Users/mtoasf/Desktop/xmly/',title))\n",
    "        return True \n",
    "    else:\n",
    "        print(u'name file exists ')\n",
    "        return False \n",
    "\n",
    "def download(url):\n",
    "    \n",
    "    content = requests.get(url).content\n",
    "    name = url.split('/')[-1]\n",
    "    with open(name,'wb') as file :\n",
    "        file.write(content)\n",
    "        \n",
    "    \n",
    "if __name__ == '__main__'  :  \n",
    "    get_detail_url()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " # Loading keys\n",
      " # Reading search terms\n",
      " # Search terms loaded\n",
      " # Search terms: flu\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR:root:caught http error 401 Client Error: Authorization Required for url: https://stream.twitter.com/1.1/statuses/filter.json on 1 try\n",
      "ERROR:root:caught http error 401 Client Error: Authorization Required for url: https://stream.twitter.com/1.1/statuses/filter.json on 2 try\n",
      "ERROR:root:caught http error 401 Client Error: Authorization Required for url: https://stream.twitter.com/1.1/statuses/filter.json on 3 try\n",
      "ERROR:root:caught http error 401 Client Error: Authorization Required for url: https://stream.twitter.com/1.1/statuses/filter.json on 4 try\n",
      "ERROR:root:caught http error 401 Client Error: Authorization Required for url: https://stream.twitter.com/1.1/statuses/filter.json on 5 try\n",
      "ERROR:root:caught http error 401 Client Error: Authorization Required for url: https://stream.twitter.com/1.1/statuses/filter.json on 6 try\n",
      "ERROR:root:caught http error 401 Client Error: Authorization Required for url: https://stream.twitter.com/1.1/statuses/filter.json on 7 try\n",
      "ERROR:root:caught http error 401 Client Error: Authorization Required for url: https://stream.twitter.com/1.1/statuses/filter.json on 8 try\n",
      "ERROR:root:caught http error 401 Client Error: Authorization Required for url: https://stream.twitter.com/1.1/statuses/filter.json on 9 try\n",
      "ERROR:root:caught http error 401 Client Error: Authorization Required for url: https://stream.twitter.com/1.1/statuses/filter.json on 10 try\n",
      "ERROR:root:caught http error 401 Client Error: Authorization Required for url: https://stream.twitter.com/1.1/statuses/filter.json on 11 try\n",
      "ERROR:root:caught http error 401 Client Error: Authorization Required for url: https://stream.twitter.com/1.1/statuses/filter.json on 12 try\n",
      "ERROR:root:caught http error 401 Client Error: Authorization Required for url: https://stream.twitter.com/1.1/statuses/filter.json on 13 try\n",
      "ERROR:root:caught http error 401 Client Error: Authorization Required for url: https://stream.twitter.com/1.1/statuses/filter.json on 14 try\n",
      "ERROR:root:caught http error 401 Client Error: Authorization Required for url: https://stream.twitter.com/1.1/statuses/filter.json on 15 try\n",
      "ERROR:root:caught http error 401 Client Error: Authorization Required for url: https://stream.twitter.com/1.1/statuses/filter.json on 16 try\n",
      "ERROR:root:caught http error 401 Client Error: Authorization Required for url: https://stream.twitter.com/1.1/statuses/filter.json on 17 try\n",
      "ERROR:root:caught http error 401 Client Error: Authorization Required for url: https://stream.twitter.com/1.1/statuses/filter.json on 18 try\n",
      "ERROR:root:caught http error 401 Client Error: Authorization Required for url: https://stream.twitter.com/1.1/statuses/filter.json on 19 try\n",
      "ERROR:root:caught http error 401 Client Error: Authorization Required for url: https://stream.twitter.com/1.1/statuses/filter.json on 20 try\n",
      "ERROR:root:caught http error 401 Client Error: Authorization Required for url: https://stream.twitter.com/1.1/statuses/filter.json on 21 try\n",
      "ERROR:root:caught http error 401 Client Error: Authorization Required for url: https://stream.twitter.com/1.1/statuses/filter.json on 22 try\n",
      "ERROR:root:caught http error 401 Client Error: Authorization Required for url: https://stream.twitter.com/1.1/statuses/filter.json on 23 try\n",
      "ERROR:root:caught http error 401 Client Error: Authorization Required for url: https://stream.twitter.com/1.1/statuses/filter.json on 24 try\n",
      "ERROR:root:caught http error 401 Client Error: Authorization Required for url: https://stream.twitter.com/1.1/statuses/filter.json on 25 try\n",
      "ERROR:root:caught http error 401 Client Error: Authorization Required for url: https://stream.twitter.com/1.1/statuses/filter.json on 26 try\n",
      "ERROR:root:caught http error 401 Client Error: Authorization Required for url: https://stream.twitter.com/1.1/statuses/filter.json on 27 try\n",
      "ERROR:root:caught http error 401 Client Error: Authorization Required for url: https://stream.twitter.com/1.1/statuses/filter.json on 28 try\n"
     ]
    }
   ],
   "source": [
    "from twarc import Twarc\n",
    "import json\n",
    "import fileinput\n",
    "import sys\n",
    "\n",
    "print (\" # Loading keys\")\n",
    "\n",
    "consumer_key = 'D8QjEPR2M0xrDhvZ0v2V21Qoc '\n",
    "consumer_secret = '8BkBP7Zu0NyGVGwDGTEehKoXODleaRotkSO1cgDAK2eC19hH1t '\n",
    "access_token = '970773468072431616-XNb3OdQONg44e9W8LIlRtfOvxeZmelv' \n",
    "access_token_secret = 'PSWZ7jTsvSQC3SLlYhCOQb2G3ExEhINTxY4LmxBzLsBeg'\n",
    "twarc_auth = Twarc(consumer_key, consumer_secret, access_token, access_token_secret)\n",
    "\n",
    "print (\" # Reading search terms\")\n",
    "\n",
    "with open('tweet_terms.txt','r') as tweet_terms_file_content:\n",
    "    my_tweet_terms = [line.strip() for line in tweet_terms_file_content]\n",
    "    print (\" # Search terms loaded\")\n",
    "    \n",
    "    if len(my_tweet_terms) > 0:\n",
    "        twitter_query = \",\".join(my_tweet_terms)\n",
    "        print (\" # Search terms: \" + twitter_query)\n",
    "\n",
    "        for tweet in twarc_auth.filter(track = twitter_query):\n",
    "            with open('data_dump.json', 'a') as json_output_file:\n",
    "                json.dump(tweet, json_output_file, indent=4, sort_keys=True)\n",
    "    else:\n",
    "        print \"No search terms provided, printing generic stream\"\n",
    "        for tweet in twarc.sample():\n",
    "            print(tweet)\n",
    "\n",
    "print (\" # Authentication successful, dumping results\")\n",
    "tweet_terms_file_content.close()\n",
    "json_output_file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
